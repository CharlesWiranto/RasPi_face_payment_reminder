{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abab0c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "894ec1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a face detector\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Generate a face recognition model\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ef5bc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to capture images and store in dataset folder\n",
    "def capture_images(User):\n",
    "\t# Create a directory to store the captured images\n",
    "\tif not os.path.exists('Faces'):\n",
    "\t\tos.makedirs('Faces')\n",
    "\n",
    "\t# Open the camera\n",
    "\tcap = cv2.VideoCapture(0)\n",
    "\n",
    "\t# Set the image counter as 0\n",
    "\tcount = 0\n",
    "\n",
    "\twhile True:\n",
    "\t\t# Read a frame from the camera\n",
    "\t\tret, frame = cap.read()\n",
    "\n",
    "\t\t# Convert the frame to grayscale\n",
    "\t\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\t\t# Detect faces in the grayscale frame\n",
    "\t\tfaces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "\t\t# Draw rectangles around the faces and store the images\n",
    "\t\tfor (x, y, w, h) in faces:\n",
    "\t\t\tcv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "\t\t\t# Store the captured face images in the Faces folder\n",
    "\t\t\tcv2.imwrite(f'Faces/{User}_{count}.jpg', gray[y:y + h, x:x + w])\n",
    "\n",
    "\t\t\tcount += 1\n",
    "\n",
    "\t\t# Display the frame with face detection\n",
    "\t\tcv2.imshow('Capture Faces', frame)\n",
    "\n",
    "\t\t# Break the loop if the 'q' key is pressed\n",
    "\t\tif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\t# Break the loop after capturing a certain number of images\n",
    "\t\tif count >= 3000:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t# Release the camera and close windows\n",
    "\tcap.release()\n",
    "\tcv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "014f0ba1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# Create the dataset of faces\n",
    "capture_images('Charles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a915985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Charles': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = {'Charles':0}\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a6f81f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.5) /io/opencv_contrib/modules/face/src/lbph_faces.cpp:362: error: (-210:Unsupported format or combination of formats) Empty training data was given. You'll need more than one sample to learn a model. in function 'train'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m recognizer\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m Recognizer \u001b[38;5;241m=\u001b[39m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m Recognizer\n",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(label)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Train the face recognition model using the faces and labels\u001b[39;00m\n\u001b[1;32m     30\u001b[0m recognizer \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mface\u001b[38;5;241m.\u001b[39mLBPHFaceRecognizer_create()\n\u001b[0;32m---> 31\u001b[0m \u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfaces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Save the trained model to a file\u001b[39;00m\n\u001b[1;32m     34\u001b[0m recognizer\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_model.xml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.5) /io/opencv_contrib/modules/face/src/lbph_faces.cpp:362: error: (-210:Unsupported format or combination of formats) Empty training data was given. You'll need more than one sample to learn a model. in function 'train'\n"
     ]
    }
   ],
   "source": [
    "def train_model(label):\n",
    "\t# Create lists to store the face samples and their corresponding labels\n",
    "\tfaces = []\n",
    "\tlabels = []\n",
    "\t\n",
    "\t# Load the images from the 'Faces' folder\n",
    "\tfor file_name in os.listdir('Faces'):\n",
    "\t\tif file_name.endswith('.jpg'):\n",
    "\t\t\t# Extract the label (person's name) from the file name\n",
    "\t\t\tname = file_name.split('_')[0]\n",
    "\t\t\t\n",
    "\t\t\t# Read the image and convert it to grayscale\n",
    "\t\t\timage = cv2.imread(os.path.join('Faces', file_name))\n",
    "\t\t\tgray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\t\t\t# Detect faces in the grayscale image\n",
    "\t\t\tdetected_faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "\t\t\t# Check if a face is detected\n",
    "\t\t\tif len(detected_faces) > 0:\n",
    "\t\t\t\t# Crop the detected face region\n",
    "\t\t\t\tface_crop = gray[detected_faces[0][1]:detected_faces[0][1] + detected_faces[0][3],\n",
    "\t\t\t\t\t\t\t\tdetected_faces[0][0]:detected_faces[0][0] + detected_faces[0][2]]\n",
    "\n",
    "\t\t\t\t# Append the face sample and label to the lists\n",
    "\t\t\t\tfaces.append(face_crop)\n",
    "\t\t\t\tlabels.append(label[name])\n",
    "\n",
    "\t# Train the face recognition model using the faces and labels\n",
    "\trecognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\trecognizer.train(faces, np.array(labels))\n",
    "\n",
    "\t# Save the trained model to a file\n",
    "\trecognizer.save('trained_model.xml')\n",
    "\treturn recognizer\n",
    "\n",
    "# Train the model\n",
    "Recognizer =train_model(label)\n",
    "Recognizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7340312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recognize faces\n",
    "def recognize_faces(recognizer, label):\n",
    "\t# Open the camera\n",
    "\tcap = cv2.VideoCapture(0)\n",
    "\t\n",
    "\t# Reverse keys and values in the dictionary\n",
    "\tlabel_name = {value: key for key, value in label.items()}\n",
    "\twhile True:\n",
    "\t\t# Read a frame from the camera\n",
    "\t\tret, frame = cap.read()\n",
    "\n",
    "\t\t# Convert the frame to grayscale\n",
    "\t\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\t\t# Detect faces in the grayscale frame\n",
    "\t\tfaces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\t\t\n",
    "\t\t# Recognize and label the faces\n",
    "\t\tfor (x, y, w, h) in faces:\n",
    "\t\t\t# Recognize the face using the trained model\n",
    "\t\t\tlabel, confidence = recognizer.predict(gray[y:y + h, x:x + w])\n",
    "\t\t\t#print(confidence)\n",
    "\t\t\tif confidence > 50:\n",
    "\t\t\t\t# Display the recognized label and confidence level\n",
    "\t\t\t\tcv2.putText(frame, label_name[label], (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\t\n",
    "\t\t\t\t# Draw a rectangle around the face\n",
    "\t\t\t\tcv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint('Unrecognized')\n",
    "\n",
    "\t\t# Display the frame with face recognition\n",
    "\t\tcv2.imshow('Recognize Faces', frame)\n",
    "\n",
    "\t\t# Break the loop if the 'q' key is pressed\n",
    "\t\tif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\t\tbreak\n",
    "\n",
    "\t# Release the camera and close windows\n",
    "\tcap.release()\n",
    "\tcv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05815a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recognize the live faces\n",
    "recognize_faces(Recognizer, label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
